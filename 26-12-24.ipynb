{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397b3e79",
   "metadata": {},
   "source": [
    "### StructType and StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e234bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType ,ArrayType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efadfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ranju\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| ID| Name|Salary|\n",
      "+---+-----+------+\n",
      "|  1| ranj| 50000|\n",
      "|  2|bobby| 80000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,\"ranj\",50000) ,(2,\"bobby\",80000)]\n",
    "\n",
    "schema = StructType([\\\n",
    "                     StructField(name=\"ID\" , dataType= IntegerType()),\\\n",
    "                     StructField(name=\"Name\" , dataType= StringType()),\\\n",
    "                     StructField(name=\"Salary\" , dataType= IntegerType())\n",
    "                    ])\n",
    "\n",
    "df = spark.createDataFrame(data,schema= schema)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d1d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4096b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32240b5e",
   "metadata": {},
   "source": [
    "### Nested Structure or Complex structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f84d677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------+\n",
      "| ID|         Name|Salary|\n",
      "+---+-------------+------+\n",
      "|  1|{ranjitha, M}| 50000|\n",
      "|  2| {Bhavana, N}| 80000|\n",
      "+---+-------------+------+\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: struct (nullable = true)\n",
      " |    |-- FirstName: string (nullable = true)\n",
      " |    |-- LastName: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "data = [(1,('ranjitha' , 'M'),50000) ,(2,('Bhavana','N'),80000)]\n",
    "\n",
    "struct = StructType([\\\n",
    "                    StructField(name=\"FirstName\", dataType=StringType()),\\\n",
    "                    StructField(name=\"LastName\" , dataType=StringType())\n",
    "                    ])\n",
    "\n",
    "schema = StructType([\\\n",
    "                     StructField(name=\"ID\" , dataType= IntegerType()),\\\n",
    "                     StructField(name=\"Name\" , dataType= struct),\\\n",
    "                     StructField(name=\"Salary\" , dataType= IntegerType())\n",
    "                    ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data,schema= schema)\n",
    "\n",
    "#display(df)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd63aa1",
   "metadata": {},
   "source": [
    "### ArrayType Column (maptype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66269ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| ID|numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "|xyz| [4, 7]|\n",
      "|pqr| [2, 6]|\n",
      "+---+-------+\n",
      "\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- numbers: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = [('abc',[1,2]), ('xyz',[4,7]), ('pqr',[2,6])]\n",
    "\n",
    "schema = StructType ([\\\n",
    "                     StructField (name=\"ID\" , dataType=StringType()),\\\n",
    "                        StructField (name=\"numbers\" , dataType=ArrayType(IntegerType()))\n",
    "                     ])\n",
    "\n",
    "df = spark.createDataFrame(data , schema= schema)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b36e8fe",
   "metadata": {},
   "source": [
    "### Accessing the Array elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845dbfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+\n",
      "| ID|numbers|first_number|\n",
      "+---+-------+------------+\n",
      "|abc| [1, 2]|           1|\n",
      "|xyz| [4, 7]|           4|\n",
      "|pqr| [2, 6]|           2|\n",
      "+---+-------+------------+\n",
      "\n",
      "+---+-------+-------------+\n",
      "| ID|numbers|second_number|\n",
      "+---+-------+-------------+\n",
      "|abc| [1, 2]|            2|\n",
      "|xyz| [4, 7]|            7|\n",
      "|pqr| [2, 6]|            6|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn(\"first_number\" , df.numbers[0]).show()\n",
    "\n",
    "df.withColumn(\"second_number\" , df.numbers[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfce986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|1st_num|2nd_num|\n",
      "+-------+-------+\n",
      "|      1|      2|\n",
      "|      3|      4|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,2),(3,4)]\n",
    "schema = (\"1st_num\",\"2nd_num\")\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf1952",
   "metadata": {},
   "source": [
    "### Combines the columns to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3cdef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|1st_num|2nd_num|numbers|\n",
      "+-------+-------+-------+\n",
      "|      1|      2| [1, 2]|\n",
      "|      3|      4| [3, 4]|\n",
      "+-------+-------+-------+\n",
      "\n",
      "root\n",
      " |-- 1st_num: long (nullable = true)\n",
      " |-- 2nd_num: long (nullable = true)\n",
      " |-- numbers: array (nullable = false)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col,array\n",
    "\n",
    "data = [(1,2),(3,4)]\n",
    "schema = (\"1st_num\",\"2nd_num\")\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df1 = df.withColumn('numbers',array(col(\"1st_num\"),(col(\"2nd_num\"))))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf060c",
   "metadata": {},
   "source": [
    "### EXPLODE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bdcaa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+\n",
      "| ID|Names|        Skills|\n",
      "+---+-----+--------------+\n",
      "|  1|  bob|[python, java]|\n",
      "|  2|  raj| [aws, dotnet]|\n",
      "+---+-----+--------------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+-----+--------------+------+\n",
      "| ID|Names|        Skills| SKILL|\n",
      "+---+-----+--------------+------+\n",
      "|  1|  bob|[python, java]|python|\n",
      "|  1|  bob|[python, java]|  java|\n",
      "|  2|  raj| [aws, dotnet]|   aws|\n",
      "|  2|  raj| [aws, dotnet]|dotnet|\n",
      "+---+-----+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col,explode\n",
    "\n",
    "data1 = [(1,'bob',['python','java']),(2,'raj',['aws','dotnet'])]\n",
    "\n",
    "schema = ['ID','Names','Skills']\n",
    "\n",
    "df = spark.createDataFrame(data1,schema)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df1 = df.withColumn('SKILL',explode(col('Skills')))\n",
    "df1 .show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa297cd9",
   "metadata": {},
   "source": [
    "### array_contain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2934b146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+------------+\n",
      "| ID|Names|        Skills|hasJavaSkill|\n",
      "+---+-----+--------------+------------+\n",
      "|  1|  bob|[python, java]|        true|\n",
      "|  2|  raj| [aws, dotnet]|       false|\n",
      "+---+-----+--------------+------------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hasJavaSkill: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col,array,array_contains\n",
    "\n",
    "data1 = [(1,'bob',['python','java']),(2,'raj',['aws','dotnet'])]\n",
    "\n",
    "schema = ['ID','Names','Skills']\n",
    "\n",
    "df = spark.createDataFrame(data1,schema)\n",
    "\n",
    "df = df.withColumn(\"hasJavaSkill\",array_contains(col('Skills'),'java'))\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7899c6f",
   "metadata": {},
   "source": [
    "### pivot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f84388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------+\n",
      "| Name|Gender|Department|Salary|\n",
      "+-----+------+----------+------+\n",
      "|Alice|  male|     Sales|  5000|\n",
      "|  Bob|female|     Sales|  4000|\n",
      "|Cathy|female|        HR|  3500|\n",
      "|David|  male|        HR|  4500|\n",
      "|  Eve|  male|        IT|  6000|\n",
      "|Frank|  male|        IT|  7000|\n",
      "+-----+------+----------+------+\n",
      "\n",
      "+----------+------+-----+\n",
      "|Department|Gender|count|\n",
      "+----------+------+-----+\n",
      "|     Sales|  male|    1|\n",
      "|     Sales|female|    1|\n",
      "|        HR|female|    1|\n",
      "|        HR|  male|    1|\n",
      "|        IT|  male|    2|\n",
      "+----------+------+-----+\n",
      "\n",
      "+----------+------+----+\n",
      "|Department|female|male|\n",
      "+----------+------+----+\n",
      "|     Sales|     1|   1|\n",
      "|        HR|     1|   1|\n",
      "|        IT|  NULL|   2|\n",
      "+----------+------+----+\n",
      "\n",
      "+----------+----+\n",
      "|Department|male|\n",
      "+----------+----+\n",
      "|     Sales|   1|\n",
      "|        HR|   1|\n",
      "|        IT|   2|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", \"male\",\"Sales\", 5000),\\\n",
    "    (\"Bob\",\"female\", \"Sales\", 4000),\\\n",
    "    (\"Cathy\",\"female\", \"HR\", 3500),\\\n",
    "    (\"David\", \"male\",\"HR\", 4500),\\\n",
    "    (\"Eve\", \"male\", \"IT\", 6000),\\\n",
    "    (\"Frank\", \"male\",\"IT\", 7000),\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Gender\" ,\"Department\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data , schema=columns)\n",
    "df.show()\n",
    "\n",
    "df.groupBy(\"Department\",\"Gender\").count().show()\n",
    "\n",
    "df.groupBy(\"Department\").pivot(\"Gender\").count().show()\n",
    "\n",
    "df.groupBy(\"Department\").pivot(\"Gender\",['male']).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d45a1d",
   "metadata": {},
   "source": [
    "### unpivot function we use Stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "947f4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|  dep|male|female|\n",
      "+-----+----+------+\n",
      "|   IT|   3|     2|\n",
      "|Sales|   3|     6|\n",
      "|   HR|   4|     3|\n",
      "+-----+----+------+\n",
      "\n",
      "+-----+------+-----+\n",
      "|  dep|gender|count|\n",
      "+-----+------+-----+\n",
      "|   IT|   abc|    3|\n",
      "|   IT|female|    2|\n",
      "|Sales|   abc|    3|\n",
      "|Sales|female|    6|\n",
      "|   HR|   abc|    4|\n",
      "|   HR|female|    3|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [('IT',3,2),('Sales',3,6),('HR',4,3)]\n",
    "\n",
    "schema = ['dep','male','female']\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "undf = df.select('dep' ,expr(\"stack(2 ,'abc',male,'female',Female) as (gender , count)\"))\n",
    "undf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97564471",
   "metadata": {},
   "source": [
    "### MAP AND FLAT MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f24a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|1st_name|2nd_name|\n",
      "+--------+--------+\n",
      "|     Bob|   Cathy|\n",
      "|   David|   Frank|\n",
      "+--------+--------+\n",
      "\n",
      "+--------+--------+-----------+\n",
      "|1st_name|2nd_name|  full_name|\n",
      "+--------+--------+-----------+\n",
      "|     Bob|   Cathy|  Bob Cathy|\n",
      "|   David|   Frank|David Frank|\n",
      "+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Bob','Cathy'),('David','Frank')]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "df = spark.createDataFrame(data, schema= ['1st_name', '2nd_name'])\n",
    "df. show()\n",
    "\n",
    "rdd1 = df.rdd.map(lambda x : x + (x[0] +' ' + x[1],))\n",
    "df1 = rdd1.toDF( ['1st_name', '2nd_name','full_name'])\n",
    "\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4692c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob Cathy\n",
      "David Frank\n",
      "['Bob', 'Cathy']\n",
      "['David', 'Frank']\n"
     ]
    }
   ],
   "source": [
    "#FlatMAp\n",
    "\n",
    "data = ['Bob Cathy','David Frank']\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "for item in rdd.collect():\n",
    "    print(item)\n",
    "    \n",
    "rdd1 = rdd.map(lambda x: x.split(' '))  #map() function\n",
    "for item in rdd1.collect():\n",
    "    print(item)\n",
    "    \n",
    "\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(' '))  #Flatmap() function\n",
    "for item in rdd2.collect():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b686c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3d1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
