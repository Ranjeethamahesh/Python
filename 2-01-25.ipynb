{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e86108",
   "metadata": {},
   "source": [
    "### BroadCasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca43559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f161d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact = 2  #small broadcast value \n",
    "\n",
    "broadcast_fact = sc.broadcast(fact)\n",
    "\n",
    "\n",
    "data = [1, 2, 3, 4, 5] #large  data \n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8353d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd = rdd.map(lambda x: x * broadcast_fact.value)\n",
    "result_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e537e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another example\n",
    "table = {'a' : 1 , 'b' : 2 , 'c' : 3}  #small dataset\n",
    "\n",
    "broadcast_v = sc.broadcast(table)\n",
    "\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081a5f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3), ('d', 0), ('e', 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_rslt = rdd.map(lambda x : (x, broadcast_v.value.get(x,0)))  #checks if x exist\n",
    "rdd_rslt.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb22651",
   "metadata": {},
   "source": [
    "### Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9c6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb61c05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = sc.accumulator(0)\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a887f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of numbers greater than 6: 4\n"
     ]
    }
   ],
   "source": [
    "value = 6\n",
    "\n",
    "\n",
    "rdd.foreach(lambda x: count.add(1) if x > value else None)\n",
    "\n",
    "print(f\"Count of numbers greater than {value}: {count.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e576f999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another example\n",
    "\n",
    "even_count = sc.accumulator(0)\n",
    "odd_count = sc.accumulator(0)\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea42105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of even numbers: 5\n",
      "Count of odd numbers: 5\n"
     ]
    }
   ],
   "source": [
    "def count_even_odd(x):\n",
    "    global even_count, odd_count\n",
    "    if x % 2 == 0:\n",
    "        even_count.add(1)\n",
    "    else:\n",
    "        odd_count.add(1)\n",
    "\n",
    "rdd.foreach(count_even_odd)\n",
    "\n",
    "print(f\"Count of even numbers: {even_count.value}\")\n",
    "print(f\"Count of odd numbers: {odd_count.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b34140",
   "metadata": {},
   "source": [
    "### when and otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a44cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ranju\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|gender|salary|\n",
      "+---+-----+------+------+\n",
      "|  1|Alice|     M|  5000|\n",
      "|  2|  Bob|     F|  4000|\n",
      "|  3|Cathy|      |  3500|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "data = [(1,\"Alice\",'M',5000),(2,\"Bob\", 'F', 4000),(3,\"Cathy\", '', 3500)]\n",
    "\n",
    "schema = ['id' ,'name' ,'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c16e25e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function when in module pyspark.sql.functions:\n",
      "\n",
      "when(condition: pyspark.sql.column.Column, value: Any) -> pyspark.sql.column.Column\n",
      "    Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "    If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "    conditions.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    condition : :class:`~pyspark.sql.Column`\n",
      "        a boolean :class:`~pyspark.sql.Column` expression.\n",
      "    value :\n",
      "        a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        column representing when expression.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.range(3)\n",
      "    >>> df.select(when(df['id'] == 2, 3).otherwise(4).alias(\"age\")).show()\n",
      "    +---+\n",
      "    |age|\n",
      "    +---+\n",
      "    |  4|\n",
      "    |  4|\n",
      "    |  3|\n",
      "    +---+\n",
      "    \n",
      "    >>> df.select(when(df.id == 2, df.id + 1).alias(\"age\")).show()\n",
      "    +----+\n",
      "    | age|\n",
      "    +----+\n",
      "    |NULL|\n",
      "    |NULL|\n",
      "    |   3|\n",
      "    +----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebbc092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| id| name| gender|\n",
      "+---+-----+-------+\n",
      "|  1|Alice|   Male|\n",
      "|  2|  Bob| Female|\n",
      "|  3|Cathy|unknown|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(df.id , df.name , when(condition= df.gender == 'M' , value='Male')\\\n",
    "               .when(condition= df.gender == 'F' , value='Female')\\\n",
    "               .otherwise('unknown').alias('gender'))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a17a37",
   "metadata": {},
   "source": [
    "### DFTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a5dadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  bob| 50000|\n",
      "|  2|Alice| 80000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'bob',50000),(2,'Alice',80000)]\n",
    "schema = ['id','name','salary']\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31857324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  BOB| 50000|\n",
      "|  2|ALICE| 80000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "df.withColumn('name' , upper(df.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7228e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converttoUpper(df):\n",
    "    return df.withColumn('name' , upper(df.name))\n",
    "\n",
    "def increaseSalary(df):\n",
    "    return df.withColumn('salary' , df.salary * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75964a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  BOB| 50000|\n",
      "|  2|ALICE| 80000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.transform(converttoUpper)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91a07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  bob|100000|\n",
      "|  2|Alice|160000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.transform(increaseSalary)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d8d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  BOB|100000|\n",
      "|  2|ALICE|160000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.transform(converttoUpper).transform(increaseSalary)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaea47a",
   "metadata": {},
   "source": [
    "### Corrupt Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be541ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+-------+\n",
      "|Month|Emp_count|Production_unit|Expense|\n",
      "+-----+---------+---------------+-------+\n",
      "|  JAN|      340|           2000|     30|\n",
      "|  FEB|      318|           4500|     29|\n",
      "|  MAR|      362|           3600|     32|\n",
      "|  APR|      348|           4800|     26|\n",
      "|  MAY|      363|           7580|     65|\n",
      "|  JUN|      435|           6478|     22|\n",
      "|  JUL|      491|           1556|     21|\n",
      "|  AUG|      255|       Thousand|     47|\n",
      "|  SEP|      258|           2900|     23|\n",
      "|  OCT|      485|           4000|     88|\n",
      "|  NOV|      155|           3400|     78|\n",
      "|  DEC|      369|           5250|     31|\n",
      "+-----+---------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header' , 'true').option('inferschema', 'true').load(r\"C:\\Users\\ranju\\Downloads\\corrput.csv\")\n",
    "#df = spark.read.csv(r\"C:\\Users\\ranju\\Downloads\\corrput.csv\" , inferSchema = True, header= True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6c603e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType , StructField, StringType,IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"Month\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"Emp_count\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"Production_unit\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"Expense\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"Corrupt_record\", dataType=StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7434a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+-------+-------------------+\n",
      "|Month|Emp_count|Production_unit|Expense|Corrupt_record     |\n",
      "+-----+---------+---------------+-------+-------------------+\n",
      "|JAN  |340      |2000           |30     |NULL               |\n",
      "|FEB  |318      |4500           |29     |NULL               |\n",
      "|MAR  |362      |3600           |32     |NULL               |\n",
      "|APR  |348      |4800           |26     |NULL               |\n",
      "|MAY  |363      |7580           |65     |NULL               |\n",
      "|JUN  |435      |6478           |22     |NULL               |\n",
      "|JUL  |491      |1556           |21     |NULL               |\n",
      "|AUG  |255      |NULL           |47     |AUG,255,Thousand,47|\n",
      "|SEP  |258      |2900           |23     |NULL               |\n",
      "|OCT  |485      |4000           |88     |NULL               |\n",
      "|NOV  |155      |3400           |78     |NULL               |\n",
      "|DEC  |369      |5250           |31     |NULL               |\n",
      "+-----+---------+---------------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PERMISSIVE mode\n",
    "df = spark.read.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('mode', 'PERMISSIVE') \\\n",
    "    .option('columnNameOfCorruptRecord', 'Corrupt_record') \\\n",
    "    .schema(schema) \\\n",
    "    .load(r\"C:\\Users\\ranju\\Downloads\\corrput.csv\")\n",
    "\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ea92db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+-------+--------------+\n",
      "|Month|Emp_count|Production_unit|Expense|Corrupt_record|\n",
      "+-----+---------+---------------+-------+--------------+\n",
      "+-----+---------+---------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_dropmalformed = spark.read.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('mode', 'DROPMALFORMED') \\\n",
    "    .schema(schema) \\\n",
    "    .load(r\"C:\\Users\\ranju\\Downloads\\corrput.csv\")\n",
    "\n",
    "# Show the cleaned data (without corrupted rows)\n",
    "df_dropmalformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96b6b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered: An error occurred while calling o359.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 199) (192.168.3.4 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///C:/Users/ranju/Downloads/corrput.csv. Details:\r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
      "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [JAN,340,2000,30,null].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\r\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\r\n",
      "\t... 22 more\r\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n",
      "\t... 28 more\r\n",
      "Caused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30\r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1394)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:333)\r\n",
      "\t... 31 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)\r\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
      "Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///C:/Users/ranju/Downloads/corrput.csv. Details:\r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n",
      "\t... 1 more\r\n",
      "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [JAN,340,2000,30,null].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\r\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\r\n",
      "\t... 22 more\r\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n",
      "\t... 28 more\r\n",
      "Caused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: JAN,340,2000,30\r\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1394)\r\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:333)\r\n",
      "\t... 31 more\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_failfast = spark.read.format('csv') \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('mode', 'FAILFAST') \\\n",
    "        .schema(schema) \\\n",
    "        .load(r\"C:\\Users\\ranju\\Downloads\\corrput.csv\")\n",
    "\n",
    "    # Show the data (if no error occurs)\n",
    "    df_failfast.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d6dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
