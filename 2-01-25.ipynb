{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e86108",
   "metadata": {},
   "source": [
    "### BroadCasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca43559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f161d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact = 2  #small broadcast value \n",
    "\n",
    "broadcast_fact = sc.broadcast(fact)\n",
    "\n",
    "\n",
    "data = [1, 2, 3, 4, 5] #large  data \n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8353d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd = rdd.map(lambda x: x * broadcast_fact.value)\n",
    "result_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e537e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another example\n",
    "table = {'a' : 1 , 'b' : 2 , 'c' : 3}  #small dataset\n",
    "\n",
    "broadcast_v = sc.broadcast(table)\n",
    "\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081a5f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3), ('d', 0), ('e', 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_rslt = rdd.map(lambda x : (x, broadcast_v.value.get(x,0)))  #checks if x exist\n",
    "rdd_rslt.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb22651",
   "metadata": {},
   "source": [
    "### Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9c6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb61c05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = sc.accumulator(0)\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a887f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of numbers greater than 6: 4\n"
     ]
    }
   ],
   "source": [
    "value = 6\n",
    "\n",
    "\n",
    "rdd.foreach(lambda x: count.add(1) if x > value else None)\n",
    "\n",
    "print(f\"Count of numbers greater than {value}: {count.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e576f999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another example\n",
    "\n",
    "even_count = sc.accumulator(0)\n",
    "odd_count = sc.accumulator(0)\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea42105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of even numbers: 5\n",
      "Count of odd numbers: 5\n"
     ]
    }
   ],
   "source": [
    "def count_even_odd(x):\n",
    "    global even_count, odd_count\n",
    "    if x % 2 == 0:\n",
    "        even_count.add(1)\n",
    "    else:\n",
    "        odd_count.add(1)\n",
    "\n",
    "rdd.foreach(count_even_odd)\n",
    "\n",
    "print(f\"Count of even numbers: {even_count.value}\")\n",
    "print(f\"Count of odd numbers: {odd_count.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b34140",
   "metadata": {},
   "source": [
    "### when and otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a44cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ranju\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|gender|salary|\n",
      "+---+-----+------+------+\n",
      "|  1|Alice|     M|  5000|\n",
      "|  2|  Bob|     F|  4000|\n",
      "|  3|Cathy|      |  3500|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "data = [(1,\"Alice\",'M',5000),(2,\"Bob\", 'F', 4000),(3,\"Cathy\", '', 3500)]\n",
    "\n",
    "schema = ['id' ,'name' ,'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c16e25e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function when in module pyspark.sql.functions:\n",
      "\n",
      "when(condition: pyspark.sql.column.Column, value: Any) -> pyspark.sql.column.Column\n",
      "    Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "    If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "    conditions.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    condition : :class:`~pyspark.sql.Column`\n",
      "        a boolean :class:`~pyspark.sql.Column` expression.\n",
      "    value :\n",
      "        a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        column representing when expression.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.range(3)\n",
      "    >>> df.select(when(df['id'] == 2, 3).otherwise(4).alias(\"age\")).show()\n",
      "    +---+\n",
      "    |age|\n",
      "    +---+\n",
      "    |  4|\n",
      "    |  4|\n",
      "    |  3|\n",
      "    +---+\n",
      "    \n",
      "    >>> df.select(when(df.id == 2, df.id + 1).alias(\"age\")).show()\n",
      "    +----+\n",
      "    | age|\n",
      "    +----+\n",
      "    |NULL|\n",
      "    |NULL|\n",
      "    |   3|\n",
      "    +----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebbc092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| id| name| gender|\n",
      "+---+-----+-------+\n",
      "|  1|Alice|   Male|\n",
      "|  2|  Bob| Female|\n",
      "|  3|Cathy|unknown|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(df.id , df.name , when(condition= df.gender == 'M' , value='Male')\\\n",
    "               .when(condition= df.gender == 'F' , value='Female')\\\n",
    "               .otherwise('unknown').alias('gender'))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a17a37",
   "metadata": {},
   "source": [
    "### DFTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a5dadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  bob| 50000|\n",
      "|  2|Alice| 80000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'bob',50000),(2,'Alice',80000)]\n",
    "schema = ['id','name','salary']\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31857324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  BOB| 50000|\n",
      "|  2|ALICE| 80000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "df.withColumn('name' , upper(df.name)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7228e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converttoUpper(df):\n",
    "    return df.withColumn('name' , upper(df.name))\n",
    "\n",
    "def increaseSalary(df):\n",
    "    return df.withColumn('salary' , df.salary * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75964a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  BOB| 50000|\n",
      "|  2|ALICE| 80000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.transform(converttoUpper)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91a07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  bob|100000|\n",
      "|  2|Alice|160000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.transform(increaseSalary)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d8d919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  BOB|100000|\n",
      "|  2|ALICE|160000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.transform(converttoUpper).transform(increaseSalary)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaea47a",
   "metadata": {},
   "source": [
    "### Corrupt Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be541ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+-------+\n",
      "|Month|Emp_count|Production_unit|Expense|\n",
      "+-----+---------+---------------+-------+\n",
      "|  JAN|      340|           2000|     30|\n",
      "|  FEB|      318|           4500|     29|\n",
      "|  MAR|      362|           3600|     32|\n",
      "|  APR|      348|           4800|     26|\n",
      "|  MAY|      363|           7580|     65|\n",
      "|  JUN|      435|           6478|     22|\n",
      "|  JUL|      491|           1556|     21|\n",
      "|  AUG|      255|       Thousand|     47|\n",
      "|  SEP|      258|           2900|     23|\n",
      "|  OCT|      485|           4000|     88|\n",
      "|  NOV|      155|           3400|     78|\n",
      "|  DEC|      369|           5250|     31|\n",
      "+-----+---------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header' , 'true').option('inferschema', 'true').load(r\"C:\\Users\\ranju\\Downloads\\corrput.csv\")\n",
    "#df = spark.read.csv(r\"C:\\Users\\ranju\\Downloads\\corrput.csv\" , inferSchema = True, header= True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6c603e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType , StructField, StringType,IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"Month\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"Emp_count\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"Production_unit\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"Expense\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"Corrupt_record\", dataType=StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2efb603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------+-------+-------------------+\n",
      "|Month|Emp_count|Production_unit|Expense|Corrupt_record     |\n",
      "+-----+---------+---------------+-------+-------------------+\n",
      "|JAN  |340      |2000           |30     |NULL               |\n",
      "|FEB  |318      |4500           |29     |NULL               |\n",
      "|MAR  |362      |3600           |32     |NULL               |\n",
      "|APR  |348      |4800           |26     |NULL               |\n",
      "|MAY  |363      |7580           |65     |NULL               |\n",
      "|JUN  |435      |6478           |22     |NULL               |\n",
      "|JUL  |491      |1556           |21     |NULL               |\n",
      "|AUG  |255      |NULL           |47     |AUG,255,Thousand,47|\n",
      "|SEP  |258      |2900           |23     |NULL               |\n",
      "|OCT  |485      |4000           |88     |NULL               |\n",
      "|NOV  |155      |3400           |78     |NULL               |\n",
      "|DEC  |369      |5250           |31     |NULL               |\n",
      "+-----+---------+---------------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PERMISSIVE mode\n",
    "df = spark.read.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('mode', 'PERMISSIVE') \\\n",
    "    .option('columnNameOfCorruptRecord', 'Corrupt_record') \\\n",
    "    .schema(schema) \\\n",
    "    .load(r\"C:\\Users\\ranju\\Downloads\\corrput.csv\")\n",
    "\n",
    "\n",
    "df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
