{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8250876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8ecff",
   "metadata": {},
   "source": [
    "### JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f70c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ranju\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------+------+\n",
      "| ID| Name|Department_ID|Salary|\n",
      "+---+-----+-------------+------+\n",
      "|  1|Alice|            1|  5000|\n",
      "|  2|  Bob|            1|  4000|\n",
      "|  3|Cathy|            2|  3500|\n",
      "|  4|David|            4|  4500|\n",
      "|  5|  Eve|            3|  6000|\n",
      "|  6|Frank|            3|  7000|\n",
      "+---+-----+-------------+------+\n",
      "\n",
      "+-------------+----------+\n",
      "|Department_ID|Department|\n",
      "+-------------+----------+\n",
      "|            1|        IT|\n",
      "|            2|        HR|\n",
      "|            3|   Payroll|\n",
      "|            5|     Sales|\n",
      "+-------------+----------+\n",
      "\n",
      "+---+-----+-------------+------+-------------+----------+\n",
      "| ID| Name|Department_ID|Salary|Department_ID|Department|\n",
      "+---+-----+-------------+------+-------------+----------+\n",
      "|  1|Alice|            1|  5000|            1|        IT|\n",
      "|  2|  Bob|            1|  4000|            1|        IT|\n",
      "|  3|Cathy|            2|  3500|            2|        HR|\n",
      "|  5|  Eve|            3|  6000|            3|   Payroll|\n",
      "|  6|Frank|            3|  7000|            3|   Payroll|\n",
      "+---+-----+-------------+------+-------------+----------+\n",
      "\n",
      "+---+-----+-------------+------+-------------+----------+\n",
      "| ID| Name|Department_ID|Salary|Department_ID|Department|\n",
      "+---+-----+-------------+------+-------------+----------+\n",
      "|  1|Alice|            1|  5000|            1|        IT|\n",
      "|  2|  Bob|            1|  4000|            1|        IT|\n",
      "|  3|Cathy|            2|  3500|            2|        HR|\n",
      "|  4|David|            4|  4500|         NULL|      NULL|\n",
      "|  5|  Eve|            3|  6000|            3|   Payroll|\n",
      "|  6|Frank|            3|  7000|            3|   Payroll|\n",
      "+---+-----+-------------+------+-------------+----------+\n",
      "\n",
      "+----+-----+-------------+------+-------------+----------+\n",
      "|  ID| Name|Department_ID|Salary|Department_ID|Department|\n",
      "+----+-----+-------------+------+-------------+----------+\n",
      "|   2|  Bob|            1|  4000|            1|        IT|\n",
      "|   1|Alice|            1|  5000|            1|        IT|\n",
      "|   3|Cathy|            2|  3500|            2|        HR|\n",
      "|   6|Frank|            3|  7000|            3|   Payroll|\n",
      "|   5|  Eve|            3|  6000|            3|   Payroll|\n",
      "|NULL| NULL|         NULL|  NULL|            5|     Sales|\n",
      "+----+-----+-------------+------+-------------+----------+\n",
      "\n",
      "+----+-----+-------------+------+-------------+----------+\n",
      "|  ID| Name|Department_ID|Salary|Department_ID|Department|\n",
      "+----+-----+-------------+------+-------------+----------+\n",
      "|   1|Alice|            1|  5000|            1|        IT|\n",
      "|   2|  Bob|            1|  4000|            1|        IT|\n",
      "|   3|Cathy|            2|  3500|            2|        HR|\n",
      "|   5|  Eve|            3|  6000|            3|   Payroll|\n",
      "|   6|Frank|            3|  7000|            3|   Payroll|\n",
      "|   4|David|            4|  4500|         NULL|      NULL|\n",
      "|NULL| NULL|         NULL|  NULL|            5|     Sales|\n",
      "+----+-----+-------------+------+-------------+----------+\n",
      "\n",
      "+---+-----+-------------+------+\n",
      "| ID| Name|Department_ID|Salary|\n",
      "+---+-----+-------------+------+\n",
      "|  1|Alice|            1|  5000|\n",
      "|  2|  Bob|            1|  4000|\n",
      "|  3|Cathy|            2|  3500|\n",
      "|  5|  Eve|            3|  6000|\n",
      "|  6|Frank|            3|  7000|\n",
      "+---+-----+-------------+------+\n",
      "\n",
      "+---+-----+-------------+------+\n",
      "| ID| Name|Department_ID|Salary|\n",
      "+---+-----+-------------+------+\n",
      "|  4|David|            4|  4500|\n",
      "+---+-----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "    (1,\"Alice\",1,5000),\\\n",
    "    (2,\"Bob\", 1, 4000),\\\n",
    "    (3,\"Cathy\", 2, 3500),\\\n",
    "    (4,\"David\",4, 4500),\\\n",
    "    (5,\"Eve\", 3, 6000),\\\n",
    "    (6,\"Frank\",3, 7000),\n",
    "]\n",
    "schema1 = [\"ID\", \"Name\" ,\"Department_ID\", \"Salary\"]\n",
    "\n",
    "empdf = spark.createDataFrame(data1 , schema=schema1)\n",
    "empdf.show()\n",
    "\n",
    "data2 = [(1,'IT'),(2,'HR'),(3,'Payroll'),(5,'Sales')]\n",
    "schema2 = [\"Department_ID\" , \"Department\"]\n",
    "\n",
    "depdf = spark.createDataFrame(data2 , schema=schema2)\n",
    "depdf.show()\n",
    "\n",
    "empdf.join(depdf, empdf.Department_ID == depdf.Department_ID, 'inner').show()\n",
    "empdf.join(depdf, empdf.Department_ID == depdf.Department_ID, 'left').show()\n",
    "empdf.join(depdf, empdf.Department_ID == depdf.Department_ID, 'right').show()\n",
    "empdf.join(depdf, empdf.Department_ID == depdf.Department_ID, 'full').show()\n",
    "empdf.join(depdf, empdf.Department_ID == depdf.Department_ID, 'leftsemi').show()\n",
    "empdf.join(depdf, empdf.Department_ID == depdf.Department_ID, 'leftanti').show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ee36cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the join\n",
      "    on : str, list or :class:`Column`, optional\n",
      "        a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    how : str, optional\n",
      "        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "        ``anti``, ``leftanti`` and ``left_anti``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        Joined DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> from pyspark.sql.functions import desc\n",
      "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
      "    >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "    >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
      "    >>> df4 = spark.createDataFrame([\n",
      "    ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "    ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "    ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "    ...     Row(age=None, height=None, name=None),\n",
      "    ... ])\n",
      "    \n",
      "    Inner join on columns (default)\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
      "    +----+------+\n",
      "    |name|height|\n",
      "    +----+------+\n",
      "    | Bob|    85|\n",
      "    +----+------+\n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    | Bob|  5|\n",
      "    +----+---+\n",
      "    \n",
      "    Outer join for both DataFrames on the 'name' column.\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
      "    ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
      "    +-----+------+\n",
      "    | name|height|\n",
      "    +-----+------+\n",
      "    |  Bob|    85|\n",
      "    |Alice|  NULL|\n",
      "    | NULL|    80|\n",
      "    +-----+------+\n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
      "    +-----+------+\n",
      "    | name|height|\n",
      "    +-----+------+\n",
      "    |  Tom|    80|\n",
      "    |  Bob|    85|\n",
      "    |Alice|  NULL|\n",
      "    +-----+------+\n",
      "    \n",
      "    Outer join for both DataFrams with multiple columns.\n",
      "    \n",
      "    >>> df.join(\n",
      "    ...     df3,\n",
      "    ...     [df.name == df3.name, df.age == df3.age],\n",
      "    ...     'outer'\n",
      "    ... ).select(df.name, df3.age).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  2|\n",
      "    |  Bob|  5|\n",
      "    +-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(empdf.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4839aa",
   "metadata": {},
   "source": [
    "### Union and UnionAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4497aa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|gender|\n",
      "+---+-----+------+\n",
      "|  1|  bob|female|\n",
      "|  2|Alice|  male|\n",
      "+---+-----+------+\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|gender|\n",
      "+---+----+------+\n",
      "|  1| bob|female|\n",
      "|  3| tom|  male|\n",
      "|  4| Eve|  male|\n",
      "+---+----+------+\n",
      "\n",
      "+---+-----+------+\n",
      "| id| name|gender|\n",
      "+---+-----+------+\n",
      "|  1|  bob|female|\n",
      "|  2|Alice|  male|\n",
      "|  1|  bob|female|\n",
      "|  3|  tom|  male|\n",
      "|  4|  Eve|  male|\n",
      "+---+-----+------+\n",
      "\n",
      "+---+-----+------+\n",
      "| id| name|gender|\n",
      "+---+-----+------+\n",
      "|  1|  bob|female|\n",
      "|  2|Alice|  male|\n",
      "|  3|  tom|  male|\n",
      "|  4|  Eve|  male|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'bob','female'),(2,'Alice','male')]\n",
    "schema1 = ['id','name','gender']\n",
    "\n",
    "df1 = spark.createDataFrame(data1,schema=schema1)\n",
    "df1.show()\n",
    "\n",
    "data2 = [(1,'bob','female'),(3,'tom','male'),(4,'Eve','male')]\n",
    "schema2 = ['id','name','gender']\n",
    "\n",
    "df2 = spark.createDataFrame(data2,schema=schema2)\n",
    "df2.show()\n",
    "\n",
    "new_df = df1.union(df2).show()\n",
    "\n",
    "new_df = df1.unionAll(df2)\n",
    "\n",
    "new_df.distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae7a580",
   "metadata": {},
   "source": [
    "### UnionByName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d4cb6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|gender|\n",
      "+---+-----+------+\n",
      "|  1|  bob|female|\n",
      "|  2|Alice|  male|\n",
      "+---+-----+------+\n",
      "\n",
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1| bob| 31|\n",
      "|  3| tom| 22|\n",
      "|  4| Eve| 25|\n",
      "+---+----+---+\n",
      "\n",
      "+---+-----+------+\n",
      "| id| name|gender|\n",
      "+---+-----+------+\n",
      "|  1|  bob|female|\n",
      "|  2|Alice|  male|\n",
      "|  1|  bob|    31|\n",
      "|  3|  tom|    22|\n",
      "|  4|  Eve|    25|\n",
      "+---+-----+------+\n",
      "\n",
      "+---+-----+------+----+\n",
      "| id| name|gender| age|\n",
      "+---+-----+------+----+\n",
      "|  1|  bob|female|NULL|\n",
      "|  2|Alice|  male|NULL|\n",
      "|  1|  bob|  NULL|  31|\n",
      "|  3|  tom|  NULL|  22|\n",
      "|  4|  Eve|  NULL|  25|\n",
      "+---+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'bob','female'),(2,'Alice','male')]\n",
    "schema1 = ['id','name','gender']\n",
    "\n",
    "df1 = spark.createDataFrame(data1,schema=schema1)\n",
    "df1.show()\n",
    "\n",
    "data2 = [(1,'bob',31),(3,'tom',22),(4,'Eve',25)]\n",
    "schema2 = ['id','name','age']\n",
    "\n",
    "df2 = spark.createDataFrame(data2,schema=schema2)\n",
    "df2.show()\n",
    "\n",
    "df1.union(df2).show()\n",
    "df1.unionByName(df2,allowMissingColumns= True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47ee96",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "229bf57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 27|\n",
      "| 30|\n",
      "| 59|\n",
      "| 70|\n",
      "| 92|\n",
      "| 96|\n",
      "| 99|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 27|\n",
      "| 30|\n",
      "| 59|\n",
      "| 70|\n",
      "| 92|\n",
      "| 96|\n",
      "| 99|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(start=1, end =101)\n",
    "\n",
    "df1 = df.sample(fraction=0.1, seed=123)\n",
    "df1.show()\n",
    "\n",
    "\n",
    "df2 = df.sample(fraction=0.1,seed=123)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c122aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485fab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cca0f688",
   "metadata": {},
   "source": [
    "### User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8765aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a141dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n",
      "| ID| Name|Salary|Bonus|\n",
      "+---+-----+------+-----+\n",
      "|  1|Alice| 80000| 2000|\n",
      "|  2|  Bob|100000| 4000|\n",
      "|  3|Cathy| 90000| 3500|\n",
      "+---+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1,\"Alice\",80000,2000),\\\n",
    "    (2,\"Bob\", 100000, 4000),\\\n",
    "    (3,\"Cathy\", 90000, 3500) ]\n",
    "schema = [\"ID\", \"Name\" , \"Salary\", \"Bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2800136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def total_sal(s,b):\n",
    "    return s+b\n",
    "\n",
    "Total = udf(lambda s,b :total_sal(s,b), returnType= IntegerType() ) #registering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f53c9063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+-------------+\n",
      "| ID| Name|Salary|Bonus|Total_payment|\n",
      "+---+-----+------+-----+-------------+\n",
      "|  1|Alice| 80000| 2000|        82000|\n",
      "|  2|  Bob|100000| 4000|       104000|\n",
      "|  3|Cathy| 90000| 3500|        93500|\n",
      "+---+-----+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Total_payment\", Total(df.Salary,df.Bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6481bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+-------------+\n",
      "| ID| Name|Salary|Bonus|Total_payment|\n",
      "+---+-----+------+-----+-------------+\n",
      "|  1|Alice| 80000| 2000|        82000|\n",
      "|  2|  Bob|100000| 4000|       104000|\n",
      "|  3|Cathy| 90000| 3500|        93500|\n",
      "+---+-----+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another way for registering\n",
    "\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def total_sal(s,b):\n",
    "    return s+b\n",
    "\n",
    "df.select('*' , total_sal(df.Salary,df.Bonus).alias(\"Total_payment\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b98602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+-------------+\n",
      "| ID| Name|Salary|Bonus|total_payment|\n",
      "+---+-----+------+-----+-------------+\n",
      "|  1|Alice| 80000| 2000|        82000|\n",
      "|  2|  Bob|100000| 4000|       104000|\n",
      "|  3|Cathy| 90000| 3500|        93500|\n",
      "+---+-----+------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1,\"Alice\",80000,2000),\\\n",
    "    (2,\"Bob\", 100000, 4000),\\\n",
    "    (3,\"Cathy\", 90000, 3500) ]\n",
    "schema = [\"ID\", \"Name\" , \"Salary\", \"Bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "def total_sal(s,b):\n",
    "    return s+b\n",
    "\n",
    "spark.udf.register(name = 'total' ,f =total_sal, returnType=IntegerType()) #registering \n",
    "\n",
    "result = spark.sql('SELECT *, total(Salary, Bonus) AS total_payment FROM df')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff36a0a",
   "metadata": {},
   "source": [
    "### Example on UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c9f835ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Category|Price|\n",
      "+--------+-----+\n",
      "|   Table|   34|\n",
      "|   Chair|   18|\n",
      "|    Lamp|   25|\n",
      "|   Couch|  120|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Table\", 34), (\"Chair\", 18), (\"Lamp\", 25), (\"Couch\", 120)]\n",
    "schema = [\"Category\", \"Price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37fd6bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------+\n",
      "|Category|Price|price_Category|\n",
      "+--------+-----+--------------+\n",
      "|   Table|   34|        Medium|\n",
      "|   Chair|   18|           Low|\n",
      "|    Lamp|   25|           Low|\n",
      "|   Couch|  120|          High|\n",
      "+--------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def categorize_price(price):\n",
    "    if price < 30:\n",
    "        return \"Low\"\n",
    "    elif price < 100:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "\n",
    "df.select('*' , categorize_price(df.Price).alias('price_Category')).s+how()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ebfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b554d761",
   "metadata": {},
   "source": [
    "### Example on Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fbf64e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "|OrderID|  Product|   Category|Price|Quantity|Region|      Date|Total_Sale|\n",
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "|   1001|    Table|Electronics|   34|       7|  East|2024-12-01|       238|\n",
      "|   1002|     Desk|Electronics|  832|       7| South|2024-12-02|      5824|\n",
      "|   1003|Headphone|  Furniture|   84|       9|  East|2024-12-03|       756|\n",
      "|   1004|    Phone|Electronics|  876|       3| North|2024-12-04|      2628|\n",
      "|   1005|    Table|  Furniture|  858|       7|  East|2024-12-05|      6006|\n",
      "|   1006|     Lamp|  Furniture|  540|       1|  West|2024-12-06|       540|\n",
      "|   1007|    Chair|  Furniture|  363|       4|  West|2024-12-07|      1452|\n",
      "|   1008|    Table|  Furniture|  148|       4| South|2024-12-08|       592|\n",
      "|   1009|Headphone|  Furniture|  667|       5|  West|2024-12-09|      3335|\n",
      "|   1010|    Phone|  Furniture|  491|       7| North|2024-12-10|      3437|\n",
      "|   1011|     Desk|  Furniture|   82|       7|  East|2024-12-11|       574|\n",
      "|   1012|Headphone|  Furniture|  158|       4| South|2024-12-12|       632|\n",
      "|   1013|Headphone|  Furniture|  518|       7|  East|2024-12-13|      3626|\n",
      "|   1014|    Chair|  Furniture|  612|       3|  East|2024-12-14|      1836|\n",
      "|   1015|  Monitor|  Furniture|  411|       6| North|2024-12-15|      2466|\n",
      "|   1016|    Phone|Electronics|  694|       2|  West|2024-12-16|      1388|\n",
      "|   1017|    Mouse|Electronics|  438|       9|  West|2024-12-17|      3942|\n",
      "|   1018|Headphone|  Furniture|  308|       5|  East|2024-12-18|      1540|\n",
      "|   1019|  Monitor|  Furniture|  398|       6| South|2024-12-19|      2388|\n",
      "|   1020|    Mouse|  Furniture|  792|       4|  West|2024-12-20|      3168|\n",
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(r\"C:\\Users\\ranju\\Downloads\\large_sales_data.csv\",inferSchema= True ,header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79b3cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "|OrderID|  Product|   Category|Price|Quantity|Region|      Date|Total_Sale|\n",
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "|   1001|    Table|Electronics|   34|       7|  East|2024-12-01|       238|\n",
      "|   1005|    Table|  Furniture|  858|       7|  East|2024-12-05|      6006|\n",
      "|   1018|Headphone|  Furniture|  308|       5|  East|2024-12-18|      1540|\n",
      "|   1020|    Mouse|  Furniture|  792|       4|  West|2024-12-20|      3168|\n",
      "|   1025|     Sofa|  Furniture|  154|       1| South|2024-12-25|       154|\n",
      "|   1026|   Laptop|Electronics|  220|       9| North|2024-12-26|      1980|\n",
      "|   1027|     Lamp|  Furniture|  859|       9| North|2024-12-27|      7731|\n",
      "|   1037|     Sofa|  Furniture|  118|       3|  West|2025-01-06|       354|\n",
      "|   1038|    Table|Electronics|  703|       8|  West|2025-01-07|      5624|\n",
      "|   1042|    Mouse|  Furniture|  980|       4| South|2025-01-11|      3920|\n",
      "|   1044|     Sofa|Electronics|  632|       1|  East|2025-01-13|       632|\n",
      "|   1045|     Lamp|  Furniture|  962|       4| South|2025-01-14|      3848|\n",
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df = df.sample(fraction=0.3 , seed =123)\n",
    "sample_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e8b9ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+-----+--------+------+----------+----------+\n",
      "|OrderID|  Product| Category|Price|Quantity|Region|      Date|Total_Sale|\n",
      "+-------+---------+---------+-----+--------+------+----------+----------+\n",
      "|   1005|    Table|Furniture|  858|       7|  East|2024-12-05|      6006|\n",
      "|   1018|Headphone|Furniture|  308|       5|  East|2024-12-18|      1540|\n",
      "|   1027|     Lamp|Furniture|  859|       9| North|2024-12-27|      7731|\n",
      "+-------+---------+---------+-----+--------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter orders where the Category is 'Furniture' and Quantity > 4\n",
    "filter_df = sample_df.filter((sample_df['Category'] == 'Furniture') & (sample_df['Quantity'] > 4))\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "57880d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|Region|sum(Total_Sale)|\n",
      "+------+---------------+\n",
      "| South|           7922|\n",
      "|  East|           8416|\n",
      "|  West|           9146|\n",
      "| North|           9711|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate total sales per region\n",
    "\n",
    "r_df = sample_df.groupBy('Region').agg(F.sum('Total_Sale'))\n",
    "r_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ef2058b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "|OrderID|  Product|   Category|Price|Quantity|Region|      Date|Total_Sale|\n",
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "|   1027|     Lamp|  Furniture|  859|       9| North|2024-12-27|      7731|\n",
      "|   1005|    Table|  Furniture|  858|       7|  East|2024-12-05|      6006|\n",
      "|   1038|    Table|Electronics|  703|       8|  West|2025-01-07|      5624|\n",
      "|   1042|    Mouse|  Furniture|  980|       4| South|2025-01-11|      3920|\n",
      "|   1045|     Lamp|  Furniture|  962|       4| South|2025-01-14|      3848|\n",
      "|   1020|    Mouse|  Furniture|  792|       4|  West|2024-12-20|      3168|\n",
      "|   1026|   Laptop|Electronics|  220|       9| North|2024-12-26|      1980|\n",
      "|   1018|Headphone|  Furniture|  308|       5|  East|2024-12-18|      1540|\n",
      "|   1044|     Sofa|Electronics|  632|       1|  East|2025-01-13|       632|\n",
      "|   1037|     Sofa|  Furniture|  118|       3|  West|2025-01-06|       354|\n",
      "|   1001|    Table|Electronics|   34|       7|  East|2024-12-01|       238|\n",
      "|   1025|     Sofa|  Furniture|  154|       1| South|2024-12-25|       154|\n",
      "+-------+---------+-----------+-----+--------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data by Total_Sale in descending order\n",
    "\n",
    "s_df = sample_df.orderBy(col('Total_Sale').desc())\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c848ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cdc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
